{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8bae680-51ee-48aa-9f49-57ea9534e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def clean_all_clams_data(directory_path):\n",
    "    '''Reformats all CLAMS data files (.csv) in the provided directory by dropping unnecessary rows.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to clean\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints new filenames saved to \"Cleaned_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    def clean_file(file_path, output_directory):\n",
    "        '''Helper function to clean individual file.'''\n",
    "        # Read the file as plain text to extract metadata\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Extract the \"Subject ID\" value\n",
    "        for line in lines:\n",
    "            if 'Subject ID' in line:\n",
    "                subject_id = line.split(',')[1].strip()\n",
    "                break\n",
    "\n",
    "        # Read the data chunk of the CSV file\n",
    "        df = pd.read_csv(file_path, skiprows=range(0, 22))\n",
    "\n",
    "        # Drop additional 2 formatting rows\n",
    "        df.drop([0, 1], inplace=True)\n",
    "\n",
    "        # Construct the new file name\n",
    "        base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "        new_file_name = f\"{base_name}_ID{subject_id}{ext}\"\n",
    "\n",
    "        # Save the cleaned data to the new directory\n",
    "        output_path = os.path.join(output_directory, new_file_name)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Cleaned CLAMS data saved to {output_path}\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory_path, \"*.CSV\"))\n",
    "    for file_path in all_files:\n",
    "        clean_file(file_path, output_directory)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs\"\n",
    "clean_all_clams_data(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a4851f4-052d-4d63-a1bc-70eeb293f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030_trimmed.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def trim_all_clams_data(directory_path, trim_hours, keep_hours):\n",
    "    '''Trims all cleaned CLAMS data files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): path to the directory containing cleaned .csv files\n",
    "    trim_hours (int): number of hours to trim from the beginning\n",
    "    keep_hours (int): number of hours to keep in the resulting file\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Saves the trimmed data to new CSV files in the \"Trimmed_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    # Create a new directory for trimmed files if it doesn't exist\n",
    "    trimmed_directory = os.path.join(directory_path, \"Trimmed_CLAMS_data\")\n",
    "    if not os.path.exists(trimmed_directory):\n",
    "        os.makedirs(trimmed_directory)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.CSV')]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        \n",
    "        # Read the cleaned CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the 'DATE/TIME' column to datetime format\n",
    "        df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'], errors='coerce')\n",
    "        \n",
    "        # Calculate the starting timestamp after trimming\n",
    "        start_time = df['DATE/TIME'].iloc[0] + timedelta(hours=trim_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the trimmed timestamp\n",
    "        df_trimmed = df[df['DATE/TIME'] >= start_time]\n",
    "        \n",
    "        # Note the value in the \"LED LIGHTNESS\" column after trimming\n",
    "        initial_led_value = df_trimmed['LED LIGHTNESS'].iloc[0]\n",
    "        \n",
    "        # Find the index of the next change in the \"LED LIGHTNESS\" value\n",
    "        led_lightness_change_index = df_trimmed[df_trimmed['LED LIGHTNESS'] != initial_led_value].index[0]\n",
    "        \n",
    "        # Calculate the ending timestamp\n",
    "        end_time = df['DATE/TIME'].iloc[led_lightness_change_index] + timedelta(hours=keep_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the LED change and end at the specified timestamp\n",
    "        df_result = df[(df['DATE/TIME'] >= df['DATE/TIME'].iloc[led_lightness_change_index]) & (df['DATE/TIME'] <= end_time)]\n",
    "        \n",
    "        # Save the resulting data to a new CSV file in the \"Trimmed_CLAMS_data\" directory\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        new_file_name = os.path.join(trimmed_directory, f\"{base_name}_trimmed{ext}\")\n",
    "        df_result.to_csv(new_file_name, index=False)\n",
    "        print(f\"Trimmed data saved to {new_file_name}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data\"\n",
    "trim_all_clams_data(directory_path, trim_hours=24, keep_hours=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "df2c6709-c56d-44a3-80d1-ade90000f516",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1594415279.py, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[92], line 90\u001b[0;36m\u001b[0m\n\u001b[0;31m    bin_clams_data(file_path, bin_hours=)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "def bin_clams_data(file_path, bin_hours):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert 'DATE/TIME' column to datetime format\n",
    "    df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\"STATUS1\", \"O2IN\", \"O2OUT\", \"DO2\", \"CO2IN\", \"CO2OUT\", \"DCO2\", \"XTOT\", \"YTOT\", \"LED HUE\", \"LED SATURATION\", \"BIN\"]\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Add TOT_AMB column to the original dataframe\n",
    "    df['TOT_AMB'] = df['XAMB'] + df['YAMB']\n",
    "    \n",
    "    # Create a new column for bin labels\n",
    "    df['BIN'] = np.nan\n",
    "    \n",
    "    # For each unique \"LED LIGHTNESS\" value, assign bin labels\n",
    "    for led_value in df['LED LIGHTNESS'].unique():\n",
    "        subset = df[df['LED LIGHTNESS'] == led_value].copy()\n",
    "        start_time = subset['DATE/TIME'].iloc[0]\n",
    "        bin_label = 0\n",
    "        bin_labels = []\n",
    "        \n",
    "        for timestamp in subset['DATE/TIME']:\n",
    "            if (timestamp - start_time) >= timedelta(hours=bin_hours):\n",
    "                bin_label += 1\n",
    "                start_time = timestamp\n",
    "            bin_labels.append(bin_label)\n",
    "        \n",
    "        df.loc[subset.index, 'BIN'] = bin_labels\n",
    "    \n",
    "    # Columns to retain the last value in the bin\n",
    "    last_val_columns = [\"INTERVAL\", \"CHAN\", \"DATE/TIME\", \"ACCO2\", \"ACCCO2\", \"FEED1 ACC\", \"WHEEL ACC\"]\n",
    "    \n",
    "    # Columns to sum within the bin\n",
    "    sum_columns = [\"WHEEL\", \"FEED1\", \"TOT_AMB\"]\n",
    "    \n",
    "    # Columns to average (excluding the ones we're taking the last value or summing)\n",
    "    avg_columns = df.columns.difference(last_val_columns + sum_columns + ['BIN', 'LED LIGHTNESS'])\n",
    "    \n",
    "    # Group by \"LED LIGHTNESS\" and \"BIN\" and calculate the mean, sum, or last value as appropriate\n",
    "    df_binned = df.groupby(['LED LIGHTNESS', 'BIN']).agg({**{col: 'last' for col in last_val_columns},\n",
    "                                                         **{col: 'mean' for col in avg_columns},\n",
    "                                                         **{col: 'sum' for col in sum_columns}}).reset_index()\n",
    "    \n",
    "    # Add start and end time columns\n",
    "    start_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].first().reset_index(name='DATE/TIME_start')\n",
    "    end_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].last().reset_index(name='DATE/TIME_end')\n",
    "    df_binned = pd.merge(df_binned, start_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Add start and end interval columns\n",
    "    start_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].first().reset_index(name='INTERVAL_start')\n",
    "    end_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].last().reset_index(name='INTERVAL_end')\n",
    "    df_binned = pd.merge(df_binned, start_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Calculate the duration of each bin in hours\n",
    "    df_binned['DURATION'] = (df_binned['DATE/TIME_end'] - df_binned['DATE/TIME_start']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Remove the BIN column and sort based on INTERVAL_start\n",
    "    df_binned = df_binned.drop(columns=['BIN']).sort_values(by='INTERVAL_start')\n",
    "    \n",
    "    # Reorder columns based on your request\n",
    "    desired_order = [\"CHAN\", \"INTERVAL_start\", \"INTERVAL_end\", \"DATE/TIME_start\", \"DATE/TIME_end\", \"DURATION\", \n",
    "                     \"VO2\", \"ACCO2\", \"VCO2\", \"ACCCO2\", \"RER\", \"HEAT\", \"FLOW\", \"PRESSURE\", \"FEED1\", \"FEED1 ACC\", \n",
    "                     \"TOT_AMB\", \"WHEEL\", \"WHEEL ACC\", \"ENCLOSURE TEMP\", \"ENCLOSURE SETPOINT\", \"LED LIGHTNESS\"]\n",
    "    df_binned = df_binned[desired_order]\n",
    "    \n",
    "    # Round all variables to 4 decimal places\n",
    "    df_binned = df_binned.round(4)\n",
    "    \n",
    "    # Save the binned data to a new CSV file\n",
    "    output_path = file_path.replace(\"Trimmed_CLAMS_data\", \"Binned_CLAMS_data\").replace(\".CSV\", \"_binned.CSV\")\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    output_directory = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    df_binned.to_csv(output_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\"\n",
    "bin_clams_data(file_path, bin_hours=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0daed0b2-07e0-4114-896e-bfe4f86b62ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     99\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mprocess_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 90\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory_path, bin_hours)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_directory\u001b[39m(directory_path, bin_hours):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Get a list of all .CSV files in the directory\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     csv_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.CSV\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, f))]\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Process each .CSV file\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "def bin_clams_data(file_path, bin_hours):\n",
    "    # ... [rest of your bin_clams_data function remains unchanged]\n",
    "\n",
    "def process_directory(directory_path, bin_hours):\n",
    "    # Get a list of all .CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.CSV') and os.path.isfile(os.path.join(directory_path, f))]\n",
    "    \n",
    "    # Process each .CSV file\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory_path, csv_file)\n",
    "        bin_clams_data(file_path, bin_hours)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data\"\n",
    "process_directory(directory_path, bin_hours=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "935f244e-eaab-4428-8054-1c384934ac6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all CLAMS data...\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371.CSV\n",
      "\n",
      "Trimming all cleaned CLAMS data...\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030_trimmed.CSV\n",
      "\n",
      "Binning all trimmed CLAMS data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     30\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmain_process_clams_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 27\u001b[0m, in \u001b[0;36mmain_process_clams_data\u001b[0;34m(directory_path, trim_hours, keep_hours, bin_hours)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBinning all trimmed CLAMS data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m trimmed_data_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned_CLAMS_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrimmed_CLAMS_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mprocess_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrimmed_data_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_hours\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 90\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory_path, bin_hours)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_directory\u001b[39m(directory_path, bin_hours):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Get a list of all .CSV files in the directory\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     csv_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.CSV\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, f))]\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Process each .CSV file\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def main_process_clams_data(directory_path, trim_hours, keep_hours, bin_hours):\n",
    "    '''Main function to process all CLAMS data files in the provided directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to process\n",
    "    trim_hours (int): number of hours to trim from the beginning of the cleaned data\n",
    "    keep_hours (int): number of hours to keep in the trimmed data\n",
    "    bin_hours (int): number of hours to bin the data\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints progress and saves processed files to respective directories.\n",
    "    '''\n",
    "    \n",
    "    print(\"Cleaning all CLAMS data...\")\n",
    "    clean_all_clams_data(directory_path)\n",
    "    \n",
    "    print(\"\\nTrimming all cleaned CLAMS data...\")\n",
    "    cleaned_data_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    trim_all_clams_data(cleaned_data_directory, trim_hours, keep_hours)\n",
    "    \n",
    "    print(\"\\nBinning all trimmed CLAMS data...\")\n",
    "    trimmed_data_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\", \"Trimmed_CLAMS_data\")\n",
    "    process_directory(trimmed_data_directory, bin_hours)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs\"\n",
    "main_process_clams_data(directory_path, trim_hours=24, keep_hours=72, bin_hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "15bf64ad-7d3a-42a4-8b90-dbf30d9ef878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all CLAMS data...\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371.CSV\n",
      "\n",
      "Trimming all cleaned CLAMS data...\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/Trimmed_CLAMS_data/2023-07-11.0105_ID3030_trimmed.CSV\n",
      "\n",
      "Binning all trimmed CLAMS data...\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Processed 2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Processed 2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Processed 2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Processed 2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Processed 2023-07-11.0105_ID3030_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def clean_all_clams_data(directory_path):\n",
    "    '''Reformats all CLAMS data files (.csv) in the provided directory by dropping unnecessary rows.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to clean\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints new filenames saved to \"Cleaned_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    def clean_file(file_path, output_directory):\n",
    "        '''Helper function to clean individual file.'''\n",
    "        # Read the file as plain text to extract metadata\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Extract the \"Subject ID\" value\n",
    "        for line in lines:\n",
    "            if 'Subject ID' in line:\n",
    "                subject_id = line.split(',')[1].strip()\n",
    "                break\n",
    "\n",
    "        # Read the data chunk of the CSV file\n",
    "        df = pd.read_csv(file_path, skiprows=range(0, 22))\n",
    "\n",
    "        # Drop additional 2 formatting rows\n",
    "        df.drop([0, 1], inplace=True)\n",
    "\n",
    "        # Construct the new file name\n",
    "        base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "        new_file_name = f\"{base_name}_ID{subject_id}{ext}\"\n",
    "\n",
    "        # Save the cleaned data to the new directory\n",
    "        output_path = os.path.join(output_directory, new_file_name)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Cleaned CLAMS data saved to {output_path}\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory_path, \"*.CSV\"))\n",
    "    for file_path in all_files:\n",
    "        clean_file(file_path, output_directory)\n",
    "\n",
    "def trim_all_clams_data(directory_path, trim_hours, keep_hours):\n",
    "    '''Trims all cleaned CLAMS data files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): path to the directory containing cleaned .csv files\n",
    "    trim_hours (int): number of hours to trim from the beginning\n",
    "    keep_hours (int): number of hours to keep in the resulting filefiles = [f for f in os.listdir(cleaned_directory) \n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Saves the trimmed data to new CSV files in the \"Trimmed_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    # Create a new directory for trimmed files if it doesn't exist\n",
    "    trimmed_directory = os.path.join(directory_path, \"Trimmed_CLAMS_data\")\n",
    "    if not os.path.exists(trimmed_directory):\n",
    "        os.makedirs(trimmed_directory)\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.CSV')]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        \n",
    "        # Read the cleaned CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the 'DATE/TIME' column to datetime format\n",
    "        df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'], errors='coerce')\n",
    "        \n",
    "        # Calculate the starting timestamp after trimming\n",
    "        start_time = df['DATE/TIME'].iloc[0] + timedelta(hours=trim_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the trimmed timestamp\n",
    "        df_trimmed = df[df['DATE/TIME'] >= start_time]\n",
    "        \n",
    "        # Note the value in the \"LED LIGHTNESS\" column after trimming\n",
    "        initial_led_value = df_trimmed['LED LIGHTNESS'].iloc[0]\n",
    "        \n",
    "        # Find the index of the next change in the \"LED LIGHTNESS\" value\n",
    "        led_lightness_change_index = df_trimmed[df_trimmed['LED LIGHTNESS'] != initial_led_value].index[0]\n",
    "        \n",
    "        # Calculate the ending timestamp\n",
    "        end_time = df['DATE/TIME'].iloc[led_lightness_change_index] + timedelta(hours=keep_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the LED change and end at the specified timestamp\n",
    "        df_result = df[(df['DATE/TIME'] >= df['DATE/TIME'].iloc[led_lightness_change_index]) & (df['DATE/TIME'] <= end_time)]\n",
    "        \n",
    "        # Save the resulting data to a new CSV file in the \"Trimmed_CLAMS_data\" directory\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        new_file_name = os.path.join(trimmed_directory, f\"{base_name}_trimmed{ext}\")\n",
    "        df_result.to_csv(new_file_name, index=False)\n",
    "        print(f\"Trimmed data saved to {new_file_name}\")\n",
    "\n",
    "def bin_clams_data(file_path, bin_hours):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert 'DATE/TIME' column to datetime format\n",
    "    df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\"STATUS1\", \"O2IN\", \"O2OUT\", \"DO2\", \"CO2IN\", \"CO2OUT\", \"DCO2\", \"XTOT\", \"YTOT\", \"LED HUE\", \"LED SATURATION\", \"BIN\"]\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Add TOT_AMB column to the original dataframe\n",
    "    df['TOT_AMB'] = df['XAMB'] + df['YAMB']\n",
    "    \n",
    "    # Create a new column for bin labels\n",
    "    df['BIN'] = np.nan\n",
    "    \n",
    "    # For each unique \"LED LIGHTNESS\" value, assign bin labels\n",
    "    for led_value in df['LED LIGHTNESS'].unique():\n",
    "        subset = df[df['LED LIGHTNESS'] == led_value].copy()\n",
    "        start_time = subset['DATE/TIME'].iloc[0]\n",
    "        bin_label = 0\n",
    "        bin_labels = []\n",
    "        \n",
    "        for timestamp in subset['DATE/TIME']:\n",
    "            if (timestamp - start_time) >= timedelta(hours=bin_hours):\n",
    "                bin_label += 1\n",
    "                start_time = timestamp\n",
    "            bin_labels.append(bin_label)\n",
    "        \n",
    "        df.loc[subset.index, 'BIN'] = bin_labels\n",
    "    \n",
    "    # Columns to retain the last value in the bin\n",
    "    last_val_columns = [\"INTERVAL\", \"CHAN\", \"DATE/TIME\", \"ACCO2\", \"ACCCO2\", \"FEED1 ACC\", \"WHEEL ACC\"]\n",
    "    \n",
    "    # Columns to sum within the bin\n",
    "    sum_columns = [\"WHEEL\", \"FEED1\", \"TOT_AMB\"]\n",
    "    \n",
    "    # Columns to average (excluding the ones we're taking the last value or summing)\n",
    "    avg_columns = df.columns.difference(last_val_columns + sum_columns + ['BIN', 'LED LIGHTNESS'])\n",
    "    \n",
    "    # Group by \"LED LIGHTNESS\" and \"BIN\" and calculate the mean, sum, or last value as appropriate\n",
    "    df_binned = df.groupby(['LED LIGHTNESS', 'BIN']).agg({**{col: 'last' for col in last_val_columns},\n",
    "                                                         **{col: 'mean' for col in avg_columns},\n",
    "                                                         **{col: 'sum' for col in sum_columns}}).reset_index()\n",
    "    \n",
    "    # Add start and end time columns\n",
    "    start_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].first().reset_index(name='DATE/TIME_start')\n",
    "    end_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].last().reset_index(name='DATE/TIME_end')\n",
    "    df_binned = pd.merge(df_binned, start_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Add start and end interval columns\n",
    "    start_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].first().reset_index(name='INTERVAL_start')\n",
    "    end_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].last().reset_index(name='INTERVAL_end')\n",
    "    df_binned = pd.merge(df_binned, start_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Calculate the duration of each bin in hours\n",
    "    df_binned['DURATION'] = (df_binned['DATE/TIME_end'] - df_binned['DATE/TIME_start']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Remove the BIN column and sort based on INTERVAL_start\n",
    "    df_binned = df_binned.drop(columns=['BIN']).sort_values(by='INTERVAL_start')\n",
    "    \n",
    "    # Reorder columns based on your request\n",
    "    desired_order = [\"CHAN\", \"INTERVAL_start\", \"INTERVAL_end\", \"DATE/TIME_start\", \"DATE/TIME_end\", \"DURATION\", \n",
    "                     \"VO2\", \"ACCO2\", \"VCO2\", \"ACCCO2\", \"RER\", \"HEAT\", \"FLOW\", \"PRESSURE\", \"FEED1\", \"FEED1 ACC\", \n",
    "                     \"TOT_AMB\", \"WHEEL\", \"WHEEL ACC\", \"ENCLOSURE TEMP\", \"ENCLOSURE SETPOINT\", \"LED LIGHTNESS\"]\n",
    "    df_binned = df_binned[desired_order]\n",
    "    \n",
    "    # Round all variables to 4 decimal places\n",
    "    df_binned = df_binned.round(4)\n",
    "    \n",
    "    # Save the binned data to a new CSV file\n",
    "    output_path = file_path.replace(\"Trimmed_CLAMS_data\", \"Binned_CLAMS_data\").replace(\".CSV\", \"_binned.CSV\")\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    output_directory = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    df_binned.to_csv(output_path, index=False)\n",
    "\n",
    "def process_directory(directory_path, bin_hours):\n",
    "    # Get a list of all .CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.CSV') and os.path.isfile(os.path.join(directory_path, f))]\n",
    "    \n",
    "    # Process each .CSV file\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory_path, csv_file)\n",
    "        bin_clams_data(file_path, bin_hours)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "def main_process_clams_data(directory_path, trim_hours, keep_hours, bin_hours):\n",
    "    '''Main function to process all CLAMS data files in the provided directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to process\n",
    "    trim_hours (int): number of hours to trim from the beginning of the cleaned data\n",
    "    keep_hours (int): number of hours to keep in the trimmed data\n",
    "    bin_hours (int): number of hours to bin the data\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints progress and saves processed files to respective directories.\n",
    "    '''\n",
    "    \n",
    "    print(\"Cleaning all CLAMS data...\")\n",
    "    clean_all_clams_data(directory_path)\n",
    "    \n",
    "    print(\"\\nTrimming all cleaned CLAMS data...\")\n",
    "    cleaned_data_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    trim_all_clams_data(cleaned_data_directory, trim_hours, keep_hours)\n",
    "    \n",
    "    print(\"\\nBinning all trimmed CLAMS data...\")\n",
    "    trimmed_data_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\", \"Trimmed_CLAMS_data\")\n",
    "    process_directory(trimmed_data_directory, bin_hours)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs\"\n",
    "main_process_clams_data(directory_path, trim_hours=24, keep_hours=72, bin_hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54bd9bfa-fc85-4546-b67c-bea31761cb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all CLAMS data...\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0103_ID2370.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0104_ID2362.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0102_ID2368.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0101_ID2364.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-11.0105_ID3030.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028.CSV\n",
      "Cleaned CLAMS data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Cleaned_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371.CSV\n",
      "\n",
      "Trimming all cleaned CLAMS data...\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Trimmed data saved to /home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs/Trimmed_CLAMS_data/2023-07-11.0105_ID3030_trimmed.CSV\n",
      "\n",
      "Binning all trimmed CLAMS data...\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0101_ID2361_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0107_ID3029_trimmed.CSV\n",
      "Processed 2023-07-11.0101_ID2364_trimmed.CSV\n",
      "Processed 2023-07-11.0103_ID2370_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0103_ID2366_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0106_ID3028_trimmed.CSV\n",
      "Processed 2023-07-11.0102_ID2368_trimmed.CSV\n",
      "Processed 2023-07-11.0104_ID2362_trimmed.CSV\n",
      "Processed 2023-07-11.0105_ID3030_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0102_ID2363_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0105_ID2365_trimmed.CSV\n",
      "Processed 2023-07-06_PEN56_Pio_Trial_1.0104_ID2371_trimmed.CSV\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def clean_all_clams_data(directory_path):\n",
    "    '''Reformats all CLAMS data files (.csv) in the provided directory by dropping unnecessary rows.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to clean\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints new filenames saved to \"Cleaned_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    def clean_file(file_path, output_directory):\n",
    "        '''Helper function to clean individual file.'''\n",
    "        # Read the file as plain text to extract metadata\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Extract the \"Subject ID\" value\n",
    "        for line in lines:\n",
    "            if 'Subject ID' in line:\n",
    "                subject_id = line.split(',')[1].strip()\n",
    "                break\n",
    "\n",
    "        # Read the data chunk of the CSV file\n",
    "        df = pd.read_csv(file_path, skiprows=range(0, 22))\n",
    "\n",
    "        # Drop additional 2 formatting rows\n",
    "        df.drop([0, 1], inplace=True)\n",
    "\n",
    "        # Construct the new file name\n",
    "        base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "        new_file_name = f\"{base_name}_ID{subject_id}{ext}\"\n",
    "\n",
    "        # Save the cleaned data to the new directory\n",
    "        output_path = os.path.join(output_directory, new_file_name)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Cleaned CLAMS data saved to {output_path}\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory_path, \"*.CSV\"))\n",
    "    for file_path in all_files:\n",
    "        clean_file(file_path, output_directory)\n",
    "\n",
    "def trim_all_clams_data(directory_path, trim_hours, keep_hours):\n",
    "    '''Trims all cleaned CLAMS data files in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): path to the directory containing cleaned .csv files\n",
    "    trim_hours (int): number of hours to trim from the beginning\n",
    "    keep_hours (int): number of hours to keep in the resulting file\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Saves the trimmed data to new CSV files in the \"Trimmed_CLAMS_data\" directory.\n",
    "    '''\n",
    "    \n",
    "    # Create a new directory for trimmed files if it doesn't exist\n",
    "    trimmed_directory = os.path.join(directory_path, \"Trimmed_CLAMS_data\")\n",
    "    if not os.path.exists(trimmed_directory):\n",
    "        os.makedirs(trimmed_directory)\n",
    "\n",
    "    # Get the path to the cleaned data files\n",
    "    cleaned_directory = os.path.join(directory_path, \"Cleaned_CLAMS_data\")\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(cleaned_directory) if os.path.isfile(os.path.join(cleaned_directory, f)) and f.endswith('.CSV')]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(cleaned_directory, file)\n",
    "        \n",
    "        # Read the cleaned CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the 'DATE/TIME' column to datetime format\n",
    "        df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'], errors='coerce')\n",
    "        \n",
    "        # Calculate the starting timestamp after trimming\n",
    "        start_time = df['DATE/TIME'].iloc[0] + timedelta(hours=trim_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the trimmed timestamp\n",
    "        df_trimmed = df[df['DATE/TIME'] >= start_time]\n",
    "        \n",
    "        # Note the value in the \"LED LIGHTNESS\" column after trimming\n",
    "        initial_led_value = df_trimmed['LED LIGHTNESS'].iloc[0]\n",
    "        \n",
    "        # Find the index of the next change in the \"LED LIGHTNESS\" value\n",
    "        led_lightness_change_index = df_trimmed[df_trimmed['LED LIGHTNESS'] != initial_led_value].index[0]\n",
    "        \n",
    "        # Calculate the ending timestamp\n",
    "        end_time = df['DATE/TIME'].iloc[led_lightness_change_index] + timedelta(hours=keep_hours)\n",
    "        \n",
    "        # Filter the dataframe to start from the LED change and end at the specified timestamp\n",
    "        df_result = df[(df['DATE/TIME'] >= df['DATE/TIME'].iloc[led_lightness_change_index]) & (df['DATE/TIME'] <= end_time)]\n",
    "        \n",
    "        # Save the resulting data to a new CSV file in the \"Trimmed_CLAMS_data\" directory\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        new_file_name = os.path.join(trimmed_directory, f\"{base_name}_trimmed{ext}\")\n",
    "        df_result.to_csv(new_file_name, index=False)\n",
    "        print(f\"Trimmed data saved to {new_file_name}\")\n",
    "\n",
    "def bin_clams_data(file_path, bin_hours):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert 'DATE/TIME' column to datetime format\n",
    "    df['DATE/TIME'] = pd.to_datetime(df['DATE/TIME'])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = [\"STATUS1\", \"O2IN\", \"O2OUT\", \"DO2\", \"CO2IN\", \"CO2OUT\", \"DCO2\", \"XTOT\", \"YTOT\", \"LED HUE\", \"LED SATURATION\", \"BIN\"]\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Add TOT_AMB column to the original dataframe\n",
    "    df['TOT_AMB'] = df['XAMB'] + df['YAMB']\n",
    "    \n",
    "    # Create a new column for bin labels\n",
    "    df['BIN'] = np.nan\n",
    "    \n",
    "    # For each unique \"LED LIGHTNESS\" value, assign bin labels\n",
    "    for led_value in df['LED LIGHTNESS'].unique():\n",
    "        subset = df[df['LED LIGHTNESS'] == led_value].copy()\n",
    "        start_time = subset['DATE/TIME'].iloc[0]\n",
    "        bin_label = 0\n",
    "        bin_labels = []\n",
    "        \n",
    "        for timestamp in subset['DATE/TIME']:\n",
    "            if (timestamp - start_time) >= timedelta(hours=bin_hours):\n",
    "                bin_label += 1\n",
    "                start_time = timestamp\n",
    "            bin_labels.append(bin_label)\n",
    "        \n",
    "        df.loc[subset.index, 'BIN'] = bin_labels\n",
    "    \n",
    "    # Columns to retain the last value in the bin\n",
    "    last_val_columns = [\"INTERVAL\", \"CHAN\", \"DATE/TIME\", \"ACCO2\", \"ACCCO2\", \"FEED1 ACC\", \"WHEEL ACC\"]\n",
    "    \n",
    "    # Columns to sum within the bin\n",
    "    sum_columns = [\"WHEEL\", \"FEED1\", \"TOT_AMB\"]\n",
    "    \n",
    "    # Columns to average (excluding the ones we're taking the last value or summing)\n",
    "    avg_columns = df.columns.difference(last_val_columns + sum_columns + ['BIN', 'LED LIGHTNESS'])\n",
    "    \n",
    "    # Group by \"LED LIGHTNESS\" and \"BIN\" and calculate the mean, sum, or last value as appropriate\n",
    "    df_binned = df.groupby(['LED LIGHTNESS', 'BIN']).agg({**{col: 'last' for col in last_val_columns},\n",
    "                                                         **{col: 'mean' for col in avg_columns},\n",
    "                                                         **{col: 'sum' for col in sum_columns}}).reset_index()\n",
    "    \n",
    "    # Add start and end time columns\n",
    "    start_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].first().reset_index(name='DATE/TIME_start')\n",
    "    end_times = df.groupby(['LED LIGHTNESS', 'BIN'])['DATE/TIME'].last().reset_index(name='DATE/TIME_end')\n",
    "    df_binned = pd.merge(df_binned, start_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_times, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Add start and end interval columns\n",
    "    start_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].first().reset_index(name='INTERVAL_start')\n",
    "    end_intervals = df.groupby(['LED LIGHTNESS', 'BIN'])['INTERVAL'].last().reset_index(name='INTERVAL_end')\n",
    "    df_binned = pd.merge(df_binned, start_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    df_binned = pd.merge(df_binned, end_intervals, on=['LED LIGHTNESS', 'BIN'])\n",
    "    \n",
    "    # Calculate the duration of each bin in hours\n",
    "    df_binned['DURATION'] = (df_binned['DATE/TIME_end'] - df_binned['DATE/TIME_start']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Remove the BIN column and sort based on INTERVAL_start\n",
    "    df_binned = df_binned.drop(columns=['BIN']).sort_values(by='INTERVAL_start')\n",
    "    \n",
    "    # Reorder columns based on your request\n",
    "    desired_order = [\"CHAN\", \"INTERVAL_start\", \"INTERVAL_end\", \"DATE/TIME_start\", \"DATE/TIME_end\", \"DURATION\", \n",
    "                     \"VO2\", \"ACCO2\", \"VCO2\", \"ACCCO2\", \"RER\", \"HEAT\", \"FLOW\", \"PRESSURE\", \"FEED1\", \"FEED1 ACC\", \n",
    "                     \"TOT_AMB\", \"WHEEL\", \"WHEEL ACC\", \"ENCLOSURE TEMP\", \"ENCLOSURE SETPOINT\", \"LED LIGHTNESS\"]\n",
    "    df_binned = df_binned[desired_order]\n",
    "    \n",
    "    # Round all variables to 4 decimal places\n",
    "    df_binned = df_binned.round(4)\n",
    "    \n",
    "    # Save the binned data to a new CSV file\n",
    "    output_path = file_path.replace(\"Trimmed_CLAMS_data\", \"Binned_CLAMS_data\").replace(\".CSV\", \"_binned.CSV\")\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    output_directory = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    df_binned.to_csv(output_path, index=False)\n",
    "\n",
    "def process_directory(directory_path, bin_hours):\n",
    "\n",
    "    # Get path to trimmed directory\n",
    "    trimmed_directory = os.path.join(directory_path, \"Trimmed_CLAMS_data\")\n",
    "    \n",
    "    # Get a list of all .CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(trimmed_directory) if f.endswith('.CSV') and os.path.isfile(os.path.join(trimmed_directory, f))]\n",
    "    \n",
    "    # Process each .CSV file\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(trimmed_directory, csv_file)\n",
    "        bin_clams_data(file_path, bin_hours)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "def main_process_clams_data(directory_path, trim_hours, keep_hours, bin_hours):\n",
    "    '''Main function to process all CLAMS data files in the provided directory.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (string): directory containing .csv files to process\n",
    "    trim_hours (int): number of hours to trim from the beginning of the cleaned data\n",
    "    keep_hours (int): number of hours to keep in the trimmed data\n",
    "    bin_hours (int): number of hours to bin the data\n",
    "    \n",
    "    Returns:\n",
    "    Nothing. Prints progress and saves processed files to respective directories.\n",
    "    '''\n",
    "    \n",
    "    print(\"Cleaning all CLAMS data...\")\n",
    "    clean_all_clams_data(directory_path)\n",
    "    \n",
    "    print(\"\\nTrimming all cleaned CLAMS data...\")\n",
    "    trim_all_clams_data(directory_path, trim_hours, keep_hours)\n",
    "    \n",
    "    print(\"\\nBinning all trimmed CLAMS data...\")\n",
    "    process_directory(directory_path, bin_hours)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/home/pistillilab/Documents/PistilliLab/Stuart Clayton/All_subjects_CSVs\"\n",
    "main_process_clams_data(directory_path, trim_hours=24, keep_hours=72, bin_hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74203eb0-dc01-4bdf-a752-5ebe3d74b297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
